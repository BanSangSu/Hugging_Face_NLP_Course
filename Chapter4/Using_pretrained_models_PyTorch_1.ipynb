{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My practices codes are in Practices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained models (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.49091124534606934, 'token': 7200, 'token_str': 'délicieux', 'sequence': 'Le camembert est délicieux :)'}, {'score': 0.10556939989328384, 'token': 2183, 'token_str': 'excellent', 'sequence': 'Le camembert est excellent :)'}, {'score': 0.03453318402171135, 'token': 26202, 'token_str': 'succulent', 'sequence': 'Le camembert est succulent :)'}, {'score': 0.033031269907951355, 'token': 528, 'token_str': 'meilleur', 'sequence': 'Le camembert est meilleur :)'}, {'score': 0.03007640317082405, 'token': 1654, 'token_str': 'parfait', 'sequence': 'Le camembert est parfait :)'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "results = camembert_fill_mask(\"Le camembert est <mask> :)\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    5,    54,   730, 25543,   110,    30, 32004,  4522,     6]])\n",
      "['<s>', '▁Le', '▁ca', 'member', 't', '▁est', '<mask>', '▁:)', '</s>']\n",
      "MaskedLMOutput(loss=None, logits=tensor([[[ 19.9792,  -4.5944,   7.1252,  ...,  -6.3530,  -3.5433,   1.3889],\n",
      "         [  0.8480,  -3.5700,  10.2163,  ...,  -9.4813,  -1.8443,  -1.3015],\n",
      "         [ -0.2291,  -9.3549,  -0.0379,  ..., -22.2491, -10.7558,  -2.2051],\n",
      "         ...,\n",
      "         [ -2.2079,  -3.1100,   0.8896,  ...,  -6.0595,  -2.9762,  -4.5024],\n",
      "         [ -1.2716,  -3.9497,   8.1755,  ...,  -2.3597,  -9.8304,  -2.0070],\n",
      "         [  5.3539,  -5.8456,  22.9419,  ...,  -8.5553,  -5.7814,   2.1215]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 9, 32005])\n",
      "torch.Size([1, 32005])\n",
      "Le camembert est délicieux :)\n",
      "Le camembert est excellent :)\n",
      "Le camembert est succulent :)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "checkpoint = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "\n",
    "# # This is the code I wrote to see result of outputs.\n",
    "# # Ref. https://huggingface.co/docs/transformers/tasks/masked_language_modeling\n",
    "# import torch\n",
    "\n",
    "# s = \"Le camembert est <mask> :)\"\n",
    "# tokenized_s = tokenizer(s, return_tensors=\"pt\")\n",
    "\n",
    "# model.eval()\n",
    "# print(tokenized_s['input_ids'])\n",
    "# print(tokenizer.convert_ids_to_tokens(tokenized_s['input_ids'][0]))\n",
    "\n",
    "# outputs = model(**tokenized_s)\n",
    "\n",
    "# print(outputs)\n",
    "# print(outputs[\"logits\"].shape)\n",
    "\n",
    "# mask_token_index = torch.where(tokenized_s[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "# mask_token_logits = outputs[\"logits\"][0, mask_token_index, :]\n",
    "# print(mask_token_logits.shape)\n",
    "# top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "# for token in top_3_tokens:\n",
    "#     print(s.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
