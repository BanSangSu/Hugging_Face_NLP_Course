{"cells":[{"cell_type":"markdown","metadata":{"id":"VU6IEp8qQFt9"},"source":["**The practice parts are what I practiced.**"]},{"cell_type":"markdown","metadata":{"id":"wyvf5oCMPt2M"},"source":["# Processing the data (PyTorch)"]},{"cell_type":"markdown","metadata":{"id":"xfiNbbZZQA8g"},"source":["## Practice"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1701012343413,"user":{"displayName":"반달곰","userId":"09517991737165413775"},"user_tz":-540},"id":"CmBqMahoQCtE","outputId":"d91fabdd-1586-4525-8583-03799135cead"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3397,"status":"ok","timestamp":1701012679353,"user":{"displayName":"반달곰","userId":"09517991737165413775"},"user_tz":-540},"id":"w22DLHRye7zg","outputId":"2bed0868-5aeb-47b1-f1de-663c31ee8cde"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","sequences = [\n","    \"I've been enjoy for studying to be a AI expert.\",\n","    \"Your objective is amazing!\",\n","]\n","batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","batch[\"labels\"] = torch.tensor([1, 1])\n","\n","optimizer = torch.optim.AdamW(model.parameters())\n","loss = model(**batch).loss\n","loss.backward()\n","optimizer.step()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1181,"status":"ok","timestamp":1701012748811,"user":{"displayName":"반달곰","userId":"09517991737165413775"},"user_tz":-540},"id":"ejrOzCxIQfdt","outputId":"1f894068-ff12-4667-c1b7-81987d09af6d"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","raw_datasets"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"NcEAUNVDQoxb"},"outputs":[{"data":{"text/plain":["{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n"," 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n"," 'label': 1,\n"," 'idx': 0}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset = raw_datasets[\"train\"]\n","raw_train_dataset[0]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["{'sentence1': Value(dtype='string', id=None),\n"," 'sentence2': Value(dtype='string', id=None),\n"," 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n"," 'idx': Value(dtype='int32', id=None)}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset.features"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'sentence1': 'Rudder was most recently senior vice president for the Developer & Platform Evangelism Business .', 'sentence2': 'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .', 'label': 0, 'idx': 16}\n","{'sentence1': 'However , EPA officials would not confirm the 20 percent figure .', 'sentence2': 'Only in the past few weeks have officials settled on the 20 percent figure .', 'label': 0, 'idx': 812}\n"]}],"source":["print(raw_train_dataset[15])\n","\n","raw_validation_dataset = raw_datasets[\"validation\"]\n","print(raw_validation_dataset[87])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","tokenized_sentence_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n","tokenized_sentence_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n","inputs"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[101, 24049, 2001, 2087, 3728, 3026, 3580, 2343, 2005, 1996, 9722, 1004, 4132, 9340, 12439, 2964, 2449, 1012, 102], [101, 3026, 3580, 2343, 4388, 24049, 1010, 3839, 2132, 1997, 1996, 9722, 1998, 4132, 9340, 12439, 2964, 3131, 1010, 2097, 2599, 1996, 2047, 9178, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","{'input_ids': [101, 24049, 2001, 2087, 3728, 3026, 3580, 2343, 2005, 1996, 9722, 1004, 4132, 9340, 12439, 2964, 2449, 1012, 102, 3026, 3580, 2343, 4388, 24049, 1010, 3839, 2132, 1997, 1996, 9722, 1998, 4132, 9340, 12439, 2964, 3131, 1010, 2097, 2599, 1996, 2047, 9178, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["raw_train_dataset_15 = raw_datasets[\"train\"][15]\n","sentence_1 = raw_train_dataset_15[\"sentence1\"]\n","sentence_2 = raw_train_dataset_15[\"sentence2\"]\n","\n","tokenized_sentence_separately = tokenizer([sentence_1, sentence_2])\n","print(tokenized_sentence_separately)\n","\n","tokenized_sentence_as_a_pair = tokenizer(sentence_1, sentence_2)\n","print(tokenized_sentence_as_a_pair)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["['[CLS]',\n"," 'this',\n"," 'is',\n"," 'the',\n"," 'first',\n"," 'sentence',\n"," '.',\n"," '[SEP]',\n"," 'this',\n"," 'is',\n"," 'the',\n"," 'second',\n"," 'one',\n"," '.',\n"," '[SEP]']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["tokenized_dataset = tokenizer(\n","    raw_datasets[\"train\"][\"sentence1\"],\n","    raw_datasets[\"train\"][\"sentence2\"],\n","    padding=True,\n","    truncation=True,\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","tokenized_datasets"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"text/plain":["[50, 59, 47, 67, 59, 50, 62, 32]"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["samples = tokenized_datasets[\"train\"][:8]\n","samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n","[len(x) for x in samples[\"input_ids\"]]"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': torch.Size([8, 67]),\n"," 'token_type_ids': torch.Size([8, 67]),\n"," 'attention_mask': torch.Size([8, 67]),\n"," 'labels': torch.Size([8])}"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["batch = data_collator(samples)\n","{k: v.shape for k, v in batch.items()}"]},{"cell_type":"markdown","metadata":{"id":"yDNUzU6PP-7_"},"source":["## Example"]},{"cell_type":"markdown","metadata":{"id":"AfAJBx0zPt2P"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mACWrnHyPt2Q"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ETDvWHwPt2R"},"outputs":[],"source":["import torch\n","from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Same as before\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","sequences = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"This course is amazing!\",\n","]\n","batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","# This is new\n","batch[\"labels\"] = torch.tensor([1, 1])\n","\n","optimizer = AdamW(model.parameters())\n","loss = model(**batch).loss\n","loss.backward()\n","optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ctuz_IbPt2S","outputId":"c4e32f6d-bb7a-4e14-d2f5-3810a0b1e2cb"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfCpNdFrPt2T","outputId":"caa04569-84e2-4cb3-969e-4a39ecdd5591"},"outputs":[{"data":{"text/plain":["{'idx': 0,\n"," 'label': 1,\n"," 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n"," 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset = raw_datasets[\"train\"]\n","raw_train_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhP5bUX0Pt2T","outputId":"26b97c92-61fa-48df-b001-a48603171479"},"outputs":[{"data":{"text/plain":["{'sentence1': Value(dtype='string', id=None),\n"," 'sentence2': Value(dtype='string', id=None),\n"," 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n"," 'idx': Value(dtype='int32', id=None)}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset.features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3roE4JCcPt2U"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n","tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqhqv7ZzPt2U","outputId":"a49b5b92-1134-4606-e5a1-8b3a5754a814"},"outputs":[{"data":{"text/plain":["{ \n","  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],\n","  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n","  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n","inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sceZkhomPt2W","outputId":"8950d780-3c7c-4271-9c9d-84438575f20f"},"outputs":[{"data":{"text/plain":["['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTd9juOQPt2X"},"outputs":[],"source":["tokenized_dataset = tokenizer(\n","    raw_datasets[\"train\"][\"sentence1\"],\n","    raw_datasets[\"train\"][\"sentence2\"],\n","    padding=True,\n","    truncation=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0ggyP5QPt2X"},"outputs":[],"source":["def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3ZGtGRYPt2X","outputId":"1916ec5f-896a-44d0-f666-46bd5c64a1d1"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPOS1vLsPt2X"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysUf--NQPt2X","outputId":"ba729827-d524-4e42-80d2-20a4f04a24c7"},"outputs":[{"data":{"text/plain":["[50, 59, 47, 67, 59, 50, 62, 32]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["samples = tokenized_datasets[\"train\"][:8]\n","samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n","[len(x) for x in samples[\"input_ids\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zcg7GYUPt2Y","outputId":"e2041f89-9806-4ca5-f3ef-8702a41f4234"},"outputs":[{"data":{"text/plain":["{'attention_mask': torch.Size([8, 67]),\n"," 'input_ids': torch.Size([8, 67]),\n"," 'token_type_ids': torch.Size([8, 67]),\n"," 'labels': torch.Size([8])}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["batch = data_collator(samples)\n","{k: v.shape for k, v in batch.items()}"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning a model with the Trainer API or Keras (PyTorch)"]},{"cell_type":"markdown","metadata":{},"source":["## Practice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Example"]},{"cell_type":"markdown","metadata":{},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\"test-trainer\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = trainer.predict(tokenized_datasets[\"validation\"])\n","print(predictions.predictions.shape, predictions.label_ids.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","preds = np.argmax(predictions.predictions, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","metric.compute(predictions=preds, references=predictions.label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    metric = evaluate.load(\"glue\", \"mrpc\")\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb","timestamp":1700911844579}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
