{"cells":[{"cell_type":"markdown","metadata":{"id":"VU6IEp8qQFt9"},"source":["**The practice parts are what I practiced.**"]},{"cell_type":"markdown","metadata":{"id":"wyvf5oCMPt2M"},"source":["# Processing the data (PyTorch)"]},{"cell_type":"markdown","metadata":{"id":"xfiNbbZZQA8g"},"source":["## Practice"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1701012343413,"user":{"displayName":"반달곰","userId":"09517991737165413775"},"user_tz":-540},"id":"CmBqMahoQCtE","outputId":"d91fabdd-1586-4525-8583-03799135cead"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3397,"status":"ok","timestamp":1701012679353,"user":{"displayName":"반달곰","userId":"09517991737165413775"},"user_tz":-540},"id":"w22DLHRye7zg","outputId":"2bed0868-5aeb-47b1-f1de-663c31ee8cde"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","sequences = [\n","    \"I've been enjoy for studying to be a AI expert.\",\n","    \"Your objective is amazing!\",\n","]\n","batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","batch[\"labels\"] = torch.tensor([1, 1])\n","\n","optimizer = torch.optim.AdamW(model.parameters())\n","loss = model(**batch).loss\n","loss.backward()\n","optimizer.step()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1181,"status":"ok","timestamp":1701012748811,"user":{"displayName":"반달곰","userId":"09517991737165413775"},"user_tz":-540},"id":"ejrOzCxIQfdt","outputId":"1f894068-ff12-4667-c1b7-81987d09af6d"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","raw_datasets"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"NcEAUNVDQoxb"},"outputs":[{"data":{"text/plain":["{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n"," 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n"," 'label': 1,\n"," 'idx': 0}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset = raw_datasets[\"train\"]\n","raw_train_dataset[0]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["{'sentence1': Value(dtype='string', id=None),\n"," 'sentence2': Value(dtype='string', id=None),\n"," 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n"," 'idx': Value(dtype='int32', id=None)}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset.features"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'sentence1': 'Rudder was most recently senior vice president for the Developer & Platform Evangelism Business .', 'sentence2': 'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .', 'label': 0, 'idx': 16}\n","{'sentence1': 'However , EPA officials would not confirm the 20 percent figure .', 'sentence2': 'Only in the past few weeks have officials settled on the 20 percent figure .', 'label': 0, 'idx': 812}\n"]}],"source":["print(raw_train_dataset[15])\n","\n","raw_validation_dataset = raw_datasets[\"validation\"]\n","print(raw_validation_dataset[87])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","tokenized_sentence_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n","tokenized_sentence_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n","inputs"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[101, 24049, 2001, 2087, 3728, 3026, 3580, 2343, 2005, 1996, 9722, 1004, 4132, 9340, 12439, 2964, 2449, 1012, 102], [101, 3026, 3580, 2343, 4388, 24049, 1010, 3839, 2132, 1997, 1996, 9722, 1998, 4132, 9340, 12439, 2964, 3131, 1010, 2097, 2599, 1996, 2047, 9178, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","{'input_ids': [101, 24049, 2001, 2087, 3728, 3026, 3580, 2343, 2005, 1996, 9722, 1004, 4132, 9340, 12439, 2964, 2449, 1012, 102, 3026, 3580, 2343, 4388, 24049, 1010, 3839, 2132, 1997, 1996, 9722, 1998, 4132, 9340, 12439, 2964, 3131, 1010, 2097, 2599, 1996, 2047, 9178, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["raw_train_dataset_15 = raw_datasets[\"train\"][15]\n","sentence_1 = raw_train_dataset_15[\"sentence1\"]\n","sentence_2 = raw_train_dataset_15[\"sentence2\"]\n","\n","tokenized_sentence_separately = tokenizer([sentence_1, sentence_2])\n","print(tokenized_sentence_separately)\n","\n","tokenized_sentence_as_a_pair = tokenizer(sentence_1, sentence_2)\n","print(tokenized_sentence_as_a_pair)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["['[CLS]',\n"," 'this',\n"," 'is',\n"," 'the',\n"," 'first',\n"," 'sentence',\n"," '.',\n"," '[SEP]',\n"," 'this',\n"," 'is',\n"," 'the',\n"," 'second',\n"," 'one',\n"," '.',\n"," '[SEP]']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["tokenized_dataset = tokenizer(\n","    raw_datasets[\"train\"][\"sentence1\"],\n","    raw_datasets[\"train\"][\"sentence2\"],\n","    padding=True,\n","    truncation=True,\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","tokenized_datasets"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"text/plain":["[50, 59, 47, 67, 59, 50, 62, 32]"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["samples = tokenized_datasets[\"train\"][:8]\n","samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n","[len(x) for x in samples[\"input_ids\"]]"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': torch.Size([8, 67]),\n"," 'token_type_ids': torch.Size([8, 67]),\n"," 'attention_mask': torch.Size([8, 67]),\n"," 'labels': torch.Size([8])}"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["batch = data_collator(samples)\n","{k: v.shape for k, v in batch.items()}"]},{"cell_type":"markdown","metadata":{"id":"yDNUzU6PP-7_"},"source":["## Example"]},{"cell_type":"markdown","metadata":{"id":"AfAJBx0zPt2P"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mACWrnHyPt2Q"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ETDvWHwPt2R"},"outputs":[],"source":["import torch\n","from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Same as before\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","sequences = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"This course is amazing!\",\n","]\n","batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","# This is new\n","batch[\"labels\"] = torch.tensor([1, 1])\n","\n","optimizer = AdamW(model.parameters())\n","loss = model(**batch).loss\n","loss.backward()\n","optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ctuz_IbPt2S","outputId":"c4e32f6d-bb7a-4e14-d2f5-3810a0b1e2cb"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfCpNdFrPt2T","outputId":"caa04569-84e2-4cb3-969e-4a39ecdd5591"},"outputs":[{"data":{"text/plain":["{'idx': 0,\n"," 'label': 1,\n"," 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n"," 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset = raw_datasets[\"train\"]\n","raw_train_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhP5bUX0Pt2T","outputId":"26b97c92-61fa-48df-b001-a48603171479"},"outputs":[{"data":{"text/plain":["{'sentence1': Value(dtype='string', id=None),\n"," 'sentence2': Value(dtype='string', id=None),\n"," 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n"," 'idx': Value(dtype='int32', id=None)}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_train_dataset.features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3roE4JCcPt2U"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n","tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqhqv7ZzPt2U","outputId":"a49b5b92-1134-4606-e5a1-8b3a5754a814"},"outputs":[{"data":{"text/plain":["{ \n","  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],\n","  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n","  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n","inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sceZkhomPt2W","outputId":"8950d780-3c7c-4271-9c9d-84438575f20f"},"outputs":[{"data":{"text/plain":["['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTd9juOQPt2X"},"outputs":[],"source":["tokenized_dataset = tokenizer(\n","    raw_datasets[\"train\"][\"sentence1\"],\n","    raw_datasets[\"train\"][\"sentence2\"],\n","    padding=True,\n","    truncation=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0ggyP5QPt2X"},"outputs":[],"source":["def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3ZGtGRYPt2X","outputId":"1916ec5f-896a-44d0-f666-46bd5c64a1d1"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n","        num_rows: 1725\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPOS1vLsPt2X"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysUf--NQPt2X","outputId":"ba729827-d524-4e42-80d2-20a4f04a24c7"},"outputs":[{"data":{"text/plain":["[50, 59, 47, 67, 59, 50, 62, 32]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["samples = tokenized_datasets[\"train\"][:8]\n","samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n","[len(x) for x in samples[\"input_ids\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zcg7GYUPt2Y","outputId":"e2041f89-9806-4ca5-f3ef-8702a41f4234"},"outputs":[{"data":{"text/plain":["{'attention_mask': torch.Size([8, 67]),\n"," 'input_ids': torch.Size([8, 67]),\n"," 'token_type_ids': torch.Size([8, 67]),\n"," 'labels': torch.Size([8])}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["batch = data_collator(samples)\n","{k: v.shape for k, v in batch.items()}"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning a model with the Trainer API or Keras (PyTorch)"]},{"cell_type":"markdown","metadata":{},"source":["## Practice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","# report_to=\"none\" ignores the wandb report.\n","training_args = TrainingArguments(\"test-trainer\", report_to=\"none\") # push_to_hub=True if you want to upload your model to the Hub during training"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1c576318428486082ee7a2d07b16cc5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1377 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.5446, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n","{'loss': 0.3298, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n","{'train_runtime': 143.0868, 'train_samples_per_second': 76.904, 'train_steps_per_second': 9.624, 'train_loss': 0.36853242318701207, 'epoch': 3.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=1377, training_loss=0.36853242318701207, metrics={'train_runtime': 143.0868, 'train_samples_per_second': 76.904, 'train_steps_per_second': 9.624, 'train_loss': 0.36853242318701207, 'epoch': 3.0})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ca40096593d48e4a524fd5fe9815326","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/51 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(408, 2) (408,)\n"]}],"source":["predictions = trainer.predict(tokenized_datasets[\"validation\"])\n","print(predictions.predictions.shape, predictions.label_ids.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","preds = np.argmax(predictions.predictions, axis=-1)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8774509803921569, 'f1': 0.9143835616438356}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","metric.compute(predictions=preds, references=predictions.label_ids)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    metric = evaluate.load(\"glue\", \"mrpc\")\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["training_args = TrainingArguments(\"test-trainer\", report_to=\"none\", evaluation_strategy=\"epoch\")\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49719ce16d3044f2b1adf137cef177ac","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1377 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1af2168be9164c5fa10f9f5bc7cc9e47","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/51 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.4733037054538727, 'eval_accuracy': 0.7230392156862745, 'eval_f1': 0.8300751879699249, 'eval_runtime': 2.5471, 'eval_samples_per_second': 160.182, 'eval_steps_per_second': 20.023, 'epoch': 1.0}\n","{'loss': 0.5822, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"123974d4de7742dfa9aca1321e1412e1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/51 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.5426512956619263, 'eval_accuracy': 0.8186274509803921, 'eval_f1': 0.8794788273615635, 'eval_runtime': 2.6572, 'eval_samples_per_second': 153.547, 'eval_steps_per_second': 19.193, 'epoch': 2.0}\n","{'loss': 0.3985, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66220342563a476191c0c8926d9cdb45","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/51 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.541414201259613, 'eval_accuracy': 0.8627450980392157, 'eval_f1': 0.904109589041096, 'eval_runtime': 2.5755, 'eval_samples_per_second': 158.415, 'eval_steps_per_second': 19.802, 'epoch': 3.0}\n","{'train_runtime': 167.5082, 'train_samples_per_second': 65.692, 'train_steps_per_second': 8.22, 'train_loss': 0.4392914228363148, 'epoch': 3.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=1377, training_loss=0.4392914228363148, metrics={'train_runtime': 167.5082, 'train_samples_per_second': 65.692, 'train_steps_per_second': 8.22, 'train_loss': 0.4392914228363148, 'epoch': 3.0})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tune a model on the GLUE SST-2 dataset"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d2af72cc3ea4e99911de973bbc8043e","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5c20867001b4134b6b491e91d6fde05","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"334918982a19439eb18f0999c3ec949d","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b691ca075d474c66a2b2166ffa967514","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"glue\", \"sst2\")\n","checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 67349\n","    })\n","    validation: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 872\n","    })\n","    test: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 1821\n","    })\n","})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def tokenize_function(example):\n","    return tokenizer(example[\"sentence\"], truncation=True)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48fc21a113504c0e91ac1ce242faf8a5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cbe2b36ac11047aa9e96c031a98da28c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/872 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25c55cb2ef7b4b4a90e22f301634ecdd","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["import evaluate\n","import numpy as np\n","\n","def compute_metrics(eval_preds):\n","    metric = evaluate.load(\"glue\", \"sst2\")\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["batch_size = 16"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    \"test-trainer\", \n","    report_to=\"none\", \n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n",")\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0f48cc32e364d399619bb430aec0787","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12630 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3464, 'learning_rate': 4.802058590657166e-05, 'epoch': 0.12}\n","{'loss': 0.2655, 'learning_rate': 4.604117181314331e-05, 'epoch': 0.24}\n","{'loss': 0.24, 'learning_rate': 4.406175771971497e-05, 'epoch': 0.36}\n","{'loss': 0.223, 'learning_rate': 4.208234362628662e-05, 'epoch': 0.48}\n","{'loss': 0.2087, 'learning_rate': 4.0102929532858274e-05, 'epoch': 0.59}\n","{'loss': 0.1979, 'learning_rate': 3.812351543942993e-05, 'epoch': 0.71}\n","{'loss': 0.1977, 'learning_rate': 3.614410134600158e-05, 'epoch': 0.83}\n","{'loss': 0.1868, 'learning_rate': 3.4164687252573244e-05, 'epoch': 0.95}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40239fbbe9ea458fa0052eaf0dcd87d6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/55 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.2728034555912018, 'eval_accuracy': 0.9036697247706422, 'eval_runtime': 2.3969, 'eval_samples_per_second': 363.796, 'eval_steps_per_second': 22.946, 'epoch': 1.0}\n","{'loss': 0.1566, 'learning_rate': 3.21852731591449e-05, 'epoch': 1.07}\n","{'loss': 0.117, 'learning_rate': 3.0205859065716553e-05, 'epoch': 1.19}\n","{'loss': 0.1127, 'learning_rate': 2.82264449722882e-05, 'epoch': 1.31}\n","{'loss': 0.1178, 'learning_rate': 2.6247030878859858e-05, 'epoch': 1.43}\n","{'loss': 0.1256, 'learning_rate': 2.4267616785431512e-05, 'epoch': 1.54}\n","{'loss': 0.1237, 'learning_rate': 2.228820269200317e-05, 'epoch': 1.66}\n","{'loss': 0.1268, 'learning_rate': 2.0308788598574824e-05, 'epoch': 1.78}\n","{'loss': 0.1158, 'learning_rate': 1.8329374505146475e-05, 'epoch': 1.9}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86031bc65a2e49c09695854bd03c506f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/55 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.3522603511810303, 'eval_accuracy': 0.9002293577981652, 'eval_runtime': 2.2349, 'eval_samples_per_second': 390.183, 'eval_steps_per_second': 24.61, 'epoch': 2.0}\n","{'loss': 0.1068, 'learning_rate': 1.6349960411718133e-05, 'epoch': 2.02}\n","{'loss': 0.0596, 'learning_rate': 1.4370546318289787e-05, 'epoch': 2.14}\n","{'loss': 0.0613, 'learning_rate': 1.2391132224861442e-05, 'epoch': 2.26}\n","{'loss': 0.0761, 'learning_rate': 1.0411718131433096e-05, 'epoch': 2.38}\n","{'loss': 0.0705, 'learning_rate': 8.432304038004752e-06, 'epoch': 2.49}\n","{'loss': 0.0663, 'learning_rate': 6.4528899445764055e-06, 'epoch': 2.61}\n","{'loss': 0.0618, 'learning_rate': 4.47347585114806e-06, 'epoch': 2.73}\n","{'loss': 0.0696, 'learning_rate': 2.494061757719715e-06, 'epoch': 2.85}\n","{'loss': 0.0704, 'learning_rate': 5.146476642913698e-07, 'epoch': 2.97}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16e6d5e9a68b4c81bd62b355d2899af9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/55 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.36852848529815674, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 2.2547, 'eval_samples_per_second': 386.755, 'eval_steps_per_second': 24.394, 'epoch': 3.0}\n","{'train_runtime': 734.8875, 'train_samples_per_second': 274.936, 'train_steps_per_second': 17.186, 'train_loss': 0.1392565613970904, 'epoch': 3.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=12630, training_loss=0.1392565613970904, metrics={'train_runtime': 734.8875, 'train_samples_per_second': 274.936, 'train_steps_per_second': 17.186, 'train_loss': 0.1392565613970904, 'epoch': 3.0})"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Example"]},{"cell_type":"markdown","metadata":{},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\"test-trainer\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = trainer.predict(tokenized_datasets[\"validation\"])\n","print(predictions.predictions.shape, predictions.label_ids.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","preds = np.argmax(predictions.predictions, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","metric.compute(predictions=preds, references=predictions.label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    metric = evaluate.load(\"glue\", \"mrpc\")\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# A full training (PyTorch)"]},{"cell_type":"markdown","metadata":{},"source":["## Practice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!pip install accelerate"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"distilbert-base-uncased\" # Remember distilbert does not use token_type_ids.\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["['labels', 'input_ids', 'attention_mask']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","tokenized_datasets[\"train\"].column_names"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",")\n","\n","eval_dataloader = DataLoader(\n","    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",")"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/plain":["{'labels': torch.Size([8]),\n"," 'input_ids': torch.Size([8, 73]),\n"," 'attention_mask': torch.Size([8, 73])}"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["for batch in train_dataloader:\n","    break\n","{k: v.shape for k, v in batch.items()}"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.6852, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"]}],"source":["outputs = model(**batch)\n","print(outputs.loss, outputs.logits.shape)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["from torch.optim import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1377\n"]}],"source":["from transformers import get_scheduler\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_sheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","print(num_training_steps)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","device"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f57a314bf755472e878e3f7947f1c829","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1377 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_sheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8480392156862745, 'f1': 0.8952702702702703}"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","model.eval()\n","for batch in eval_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    \n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"]},{"cell_type":"markdown","metadata":{},"source":["### Modify the previous training loop to fine-tune your model on the SST-2 dataset."]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 67349\n","    })\n","    validation: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 872\n","    })\n","    test: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 1821\n","    })\n","})"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"sst2\")\n","checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","raw_datasets"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c958769125e645418c87f5cc2c899ce7","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n","        num_rows: 67349\n","    })\n","    validation: Dataset({\n","        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n","        num_rows: 872\n","    })\n","    test: Dataset({\n","        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n","        num_rows: 1821\n","    })\n","})"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["def tokenize_function(example):\n","    return tokenizer(example[\"sentence\"], truncation=True)\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","tokenized_datasets"]},{"cell_type":"markdown","metadata":{},"source":["tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","tokenized_datasets[\"train\"].column_names"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",")\n","eval_dataloader = DataLoader(\n","    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",")"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["25257"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoModelForSequenceClassification, get_scheduler\n","from torch.optim import AdamW\n","import torch\n","import evaluate\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","num_training_steps"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","device"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f13d151d5d045868318bacc069a9093","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25257 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.8956422018348624}\n"]}],"source":["from tqdm.auto import tqdm\n","\n","def train():\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for batch in train_dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            loss.backward()\n","\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","\n","def eval():\n","    metric = evaluate.load(\"glue\", \"sst2\")\n","    model.eval()\n","    for batch in eval_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","    result = metric.compute()\n","    print(result)\n","\n","progress_bar = tqdm(range(num_training_steps))\n","train()\n","eval()"]},{"cell_type":"markdown","metadata":{},"source":["-------"]},{"cell_type":"markdown","metadata":{},"source":["### Accelerate"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ae992f3c5794dadb1950cc9b9a50be5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1377 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}],"source":["from torch.optim import AdamW\n","from transformers import AutoModelForSequenceClassification, get_scheduler\n","\n","import torch\n","from tqdm.auto import tqdm\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6543a271c79b44ee905efbb5f5d7f50a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1377 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5815940d6508455fa696685723dc1582","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/51 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.8406862745098039, 'f1': 0.8903878583473862}\n"]}],"source":["from accelerate import Accelerator\n","from transformers import AutoModelForSequenceClassification, get_scheduler\n","from accelerate.utils import set_seed\n","\n","import evaluate\n","from torch.optim import AdamW\n","import torch\n","from tqdm.auto import tqdm\n","\n","\n","def eval(model, eval_dl):\n","    metric = evaluate.load(\"glue\", \"mrpc\")\n","    model.eval()\n","    for batch in tqdm(eval_dl):\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","    result = metric.compute()\n","    print(result)\n","\n","def train(mixed_precision:str=\"fp16\", seed:int=42):\n","    set_seed(seed)\n","\n","    accelerator = Accelerator(mixed_precision=mixed_precision)\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","    optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","    train_dl, eval_dl, model, optimizer = accelerator.prepare(\n","        train_dataloader, eval_dataloader, model, optimizer\n","    )\n","\n","    num_epochs = 3\n","    num_training_steps = num_epochs * len(train_dl)\n","    lr_scheduler = get_scheduler(\n","        \"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_training_steps,\n","    )\n","\n","    progress_bar = tqdm(range(num_training_steps))\n","    \n","    model.train()\n","    for epoch in range(num_epochs):\n","        for batch in train_dl:\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            accelerator.backward(loss)\n","\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","\n","    eval(model, eval_dl)\n","\n","\n","# args = (\"fp16\", 42)\n","# train(*args)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Launching training on one GPU.\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6db7ac611f10451888e8ad684e1f7751","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1377 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04a68148839147fd8066fe2badb57c33","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/51 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.8406862745098039, 'f1': 0.8903878583473862}\n"]}],"source":["from accelerate import notebook_launcher\n","\n","# When you use the notebook_launcher, you need to pass these inputs(args and num_processes). \n","args = (\"fp16\", 42)\n","notebook_launcher(train, args, num_processes=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Example"]},{"cell_type":"markdown","metadata":{},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!pip install accelerate\n","# To run the training on TPU, you will need to uncomment the following line:\n","# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","tokenized_datasets[\"train\"].column_names"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",")\n","eval_dataloader = DataLoader(\n","    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for batch in train_dataloader:\n","    break\n","{k: v.shape for k, v in batch.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs = model(**batch)\n","print(outputs.loss, outputs.logits.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import get_scheduler\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","print(num_training_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","model.eval()\n","for batch in eval_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from accelerate import Accelerator\n","from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n","\n","accelerator = Accelerator()\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","train_dl, eval_dl, model, optimizer = accelerator.prepare(\n","    train_dataloader, eval_dataloader, model, optimizer\n",")\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dl)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dl:\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from accelerate import notebook_launcher\n","\n","notebook_launcher(training_function)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb","timestamp":1700911844579}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
