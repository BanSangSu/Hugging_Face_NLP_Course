{"cells":[{"cell_type":"markdown","source":["**I practiced the codes in practice part.**"],"metadata":{"id":"6RCJKKaianrJ"}},{"cell_type":"markdown","metadata":{"id":"VKEx61-5JW3k"},"source":["# Behind the pipeline (PyTorch)"]},{"cell_type":"markdown","source":["## Practice"],"metadata":{"id":"F9e7ZdTO_wcd"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]"],"metadata":{"id":"iQOksABXXA0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","pipeline_result = classifier(\n","    [\n","        \"I've been waiting for you my whole life.\",\n","        \"I hate this so much!\",\n","    ]\n",")\n","print(pipeline_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSzuqvyn-Ute","executionInfo":{"status":"ok","timestamp":1700405991223,"user_tz":-540,"elapsed":1681,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"f759ddc7-91e2-4499-850e-73f7c7278fe0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9992159605026245}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"],"metadata":{"id":"X6cjJnqx-9sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_inputs = [\n","        \"I've been waiting for you my whole life.\",\n","        \"I hate this so much!\",\n","]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcPvbGHvBKR4","executionInfo":{"status":"ok","timestamp":1700404824788,"user_tz":-540,"elapsed":431,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"045fcc30-106d-4b99-9283-7c50b0ceeaa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 2017, 2026, 2878, 2166, 1012,\n","          102],\n","        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,    0,\n","            0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n"]}]},{"cell_type":"code","source":["from transformers import AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModel.from_pretrained(checkpoint)"],"metadata":{"id":"A0uht1BFBtVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duY4gZd-B8HZ","executionInfo":{"status":"ok","timestamp":1700404947582,"user_tz":-540,"elapsed":453,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"0aa2b0c9-4b61-457d-fbb2-c23107d4fbf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 13, 768])\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs = model(**inputs)"],"metadata":{"id":"lTC4WTUICHAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs.logits.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fTCVRYVOJGcW","executionInfo":{"status":"ok","timestamp":1700406791398,"user_tz":-540,"elapsed":3,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"9df5745b-8fe6-4314-b1a1-86997d800a4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2])\n"]}]},{"cell_type":"code","source":["print(outputs.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAMVUF5rCHVR","executionInfo":{"status":"ok","timestamp":1700405241782,"user_tz":-540,"elapsed":266,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"21574fb2-c576-4771-a48f-0cbda9562b3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-3.5103,  3.6400],\n","        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["import torch\n","\n","predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","print(predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mx5DdlgCHmu","executionInfo":{"status":"ok","timestamp":1700405578081,"user_tz":-540,"elapsed":452,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"f10fbaf1-2811-4605-9a47-d35293c3560c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[7.8402e-04, 9.9922e-01],\n","        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["model.config.id2label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"utSCGxUFDP4_","executionInfo":{"status":"ok","timestamp":1700405357572,"user_tz":-540,"elapsed":263,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"a405fe89-12a1-4cee-da94-b7702a79fbaa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'NEGATIVE', 1: 'POSITIVE'}"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["step_results = []\n","\n","for prediction in predictions:\n","    label = \"\"\n","    score = 0\n","    if prediction[0] > prediction[1]:\n","        label = \"NEGATIVE\"\n","        score = prediction[0]\n","    else:\n","        label = \"POSITIVE\"\n","        score = prediction[1]\n","    temp_dict = {}\n","    temp_dict[\"label\"] = label\n","    temp_dict[\"score\"] = score.item()\n","    step_results.append(temp_dict)\n","print(step_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PoPtWfSGEDem","executionInfo":{"status":"ok","timestamp":1700406706664,"user_tz":-540,"elapsed":419,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"be4f7e69-aca6-4e4d-ac9c-94d65b0907d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9992159605026245}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n"]}]},{"cell_type":"code","source":["print(step_results == pipeline_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaCF5rhVGJI-","executionInfo":{"status":"ok","timestamp":1700406714305,"user_tz":-540,"elapsed":402,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"ab53d61e-af7e-4761-9d4d-9cdcbad4180a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"markdown","source":["## Example"],"metadata":{"id":"144ULNhu_31l"}},{"cell_type":"markdown","metadata":{"id":"Zs4OF8LxJW3o"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4-ESLc8JW3o"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dLZlH0-JW3q","outputId":"eaa0a5f0-7e18-40b7-b3c2-03fd247c8cb8"},"outputs":[{"data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n"," {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\n","    [\n","        \"I've been waiting for a HuggingFace course my whole life.\",\n","        \"I hate this so much!\",\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhTUiEhsJW3r"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hparvHZFJW3r","outputId":"f3894a17-2ee2-4a41-c033-e1e9d1f314f8"},"outputs":[{"data":{"text/plain":["{\n","    'input_ids': tensor([\n","        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n","        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n","    ]), \n","    'attention_mask': tensor([\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n","    ])\n","}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_inputs = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVV939xnJW3s"},"outputs":[],"source":["from transformers import AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModel.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5se4j2XgJW3s","outputId":"80a01881-ad9d-4d02-9fca-6b42a3593a28"},"outputs":[{"data":{"text/plain":["torch.Size([2, 16, 768])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_KpJ7WgJW3t"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs = model(**inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHbiegijJW3t","outputId":"01215bbe-89a2-49be-9559-6745ee437e4d"},"outputs":[{"data":{"text/plain":["torch.Size([2, 2])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(outputs.logits.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHLgB-GZJW3u","outputId":"3c1a5705-b4fc-469b-bafd-27671e155111"},"outputs":[{"data":{"text/plain":["tensor([[-1.5607,  1.6123],\n","        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(outputs.logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rj7TzPASJW3u","outputId":"15dd19b7-30ff-4869-e2fb-b0c2fc39676a"},"outputs":[{"data":{"text/plain":["tensor([[4.0195e-02, 9.5980e-01],\n","        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ikqjj6KLJW3u","outputId":"5446f378-e80c-41ba-be89-fcc9ccbd96a8"},"outputs":[{"data":{"text/plain":["{0: 'NEGATIVE', 1: 'POSITIVE'}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model.config.id2label"]},{"cell_type":"markdown","metadata":{"id":"fT1vAdGZZNsn"},"source":["# Models (PyTorch)"]},{"cell_type":"markdown","source":["## Practice"],"metadata":{"id":"ZT0WO2-38TSc"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]"],"metadata":{"id":"uw7At_3b8Wey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertConfig, BertModel\n","\n","config = BertConfig()\n","model = BertModel(config)"],"metadata":{"id":"-dK0MCEfOiVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(config)"],"metadata":{"id":"Cmfkhd7zPvll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertConfig, BertModel\n","\n","config = BertConfig()\n","model = BertModel(config)\n","print(model)"],"metadata":{"id":"zN8EcUlJPxP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertConfig, BertModel\n","my_bert_config = BertConfig.from_pretrained(\"bert-base-cased\", num_hidden_layers=10)\n","model = BertModel(my_bert_config)"],"metadata":{"id":"q4gH4AC2P_QY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(my_bert_config)"],"metadata":{"id":"12GCNgSkSDpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save_pretrained(\"my-bert-model\")"],"metadata":{"id":"s-5iRNMcQaBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel\n","\n","bert_model = BertModel.from_pretrained(\"my-bert-model\")"],"metadata":{"id":"97C6F-DjRduL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(bert_model.config)"],"metadata":{"id":"djUNVVhPTIuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences = [\"Hello!\", \"Cool.\", \"Nice?\"]"],"metadata":{"id":"Z84MsQvAQmoX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_sequences = [\n","    [101, 5, 999, 102],\n","    [101, 2434, 543, 102],\n","    [101, 3835, 999, 102],\n","]"],"metadata":{"id":"QPnzspIkQsCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","model_inputs = torch.tensor(encoded_sequences)"],"metadata":{"id":"a9w4EE8UQ1E3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = model(model_inputs)"],"metadata":{"id":"wUc6sf5hQ5fR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example"],"metadata":{"id":"6O9hyBnHAQeE"}},{"cell_type":"markdown","metadata":{"id":"ZrOLA3Y8ZNsr"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7iwQ9OXZNss"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTMbN9vOZNst"},"outputs":[],"source":["from transformers import BertConfig, BertModel\n","\n","# Building the config\n","config = BertConfig()\n","\n","# Building the model from the config\n","model = BertModel(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMhUP7MsZNst","outputId":"07a34688-9f4e-4777-d8ff-436e6430d783"},"outputs":[{"data":{"text/plain":["BertConfig {\n","  [...]\n","  \"hidden_size\": 768,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  [...]\n","}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilB7fUN4ZNsu"},"outputs":[],"source":["from transformers import BertConfig, BertModel\n","\n","config = BertConfig()\n","model = BertModel(config)\n","\n","# Model is randomly initialized!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AodCpUAZNsv"},"outputs":[],"source":["from transformers import BertModel\n","\n","model = BertModel.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aer-HC-ZNsv"},"outputs":[],"source":["model.save_pretrained(\"directory_on_my_computer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hrnw7sEZNsw"},"outputs":[],"source":["sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxzvevpUZNsw"},"outputs":[],"source":["encoded_sequences = [\n","    [101, 7592, 999, 102],\n","    [101, 4658, 1012, 102],\n","    [101, 3835, 999, 102],\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"246G0NqUZNsw"},"outputs":[],"source":["import torch\n","\n","model_inputs = torch.tensor(encoded_sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Pg_1kTbZNsw"},"outputs":[],"source":["output = model(model_inputs)"]},{"cell_type":"markdown","metadata":{"id":"2H2RFdqlZZT0"},"source":["# Tokenizers (PyTorch)"]},{"cell_type":"markdown","source":["## Practice"],"metadata":{"id":"k75IvrBAUL5f"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]"],"metadata":{"id":"pA6dkAaRUNWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_text = \"Super Kimbob was a delicious food.\".split()\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FL0V9S0zUTCx","executionInfo":{"status":"ok","timestamp":1700830320085,"user_tz":-540,"elapsed":4,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"7fd67027-8197-481c-cb91-0683d15ee418"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Super', 'Kimbob', 'was', 'a', 'delicious', 'food.']\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"Aprglh9PYyaq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mXeoRKIZFRM","executionInfo":{"status":"ok","timestamp":1700830412377,"user_tz":-540,"elapsed":879,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"51143999-6281-45b5-a1cf-9bea71434c38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"ZMLB44ATZJJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFDXnaDnZOIb","executionInfo":{"status":"ok","timestamp":1700830442322,"user_tz":-540,"elapsed":3,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"e8d5c466-4b26-4ef9-fc3f-a01fca5df2da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n"]}]},{"cell_type":"code","source":["tokenizer(\"Using huggingface library is very easy.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEIVGh_7ZO2k","executionInfo":{"status":"ok","timestamp":1700830482862,"user_tz":-540,"elapsed":692,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"f8ce18ec-dd61-49de-cd60-5cb241a4dbec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 7993, 19558, 10931, 3340, 1110, 1304, 3123, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sequence = \"Using huggingface library is very easy.\"\n","tokens = tokenizer.tokenize(sequence)\n","\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KnpFtEdaBGq","executionInfo":{"status":"ok","timestamp":1700830735708,"user_tz":-540,"elapsed":801,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"81c64cee-ef25-4379-e705-d410de915c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Using', 'hugging', '##face', 'library', 'is', 'very', 'easy', '.']\n"]}]},{"cell_type":"code","source":["ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16rnChcaaf6V","executionInfo":{"status":"ok","timestamp":1700830793569,"user_tz":-540,"elapsed":2,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"2c1bdf1c-018f-4d92-ef35-be1399b7845c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[7993, 19558, 10931, 3340, 1110, 1304, 3123, 119]\n"]}]},{"cell_type":"code","source":["decoded_string = tokenizer.decode([7993, 19558, 10931, 3340, 1110, 1304, 3123, 119])\n","\n","print(decoded_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJvEsDivalOw","executionInfo":{"status":"ok","timestamp":1700830937095,"user_tz":-540,"elapsed":629,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"f586f725-6fae-4fa6-9327-9bf0aca95e4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using huggingface library is very easy.\n"]}]},{"cell_type":"markdown","source":["## Example"],"metadata":{"id":"gSWJqxMPAUfx"}},{"cell_type":"markdown","metadata":{"id":"rBPFNpL7ZZT3"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwCkJArXZZT4"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIwEdkaKZZT5","outputId":"48e93949-3394-4817-8650-8b66891fb9bd"},"outputs":[{"data":{"text/plain":["['Jim', 'Henson', 'was', 'a', 'puppeteer']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_text = \"Jim Henson was a puppeteer\".split()\n","print(tokenized_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eT_VC_a1ZZT6"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oY9S_sIKZZT7"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lojRBerIZZT7","outputId":"c8192091-80f2-4ebd-a431-7412fb83cbaa"},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],\n"," 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Using a Transformer network is simple\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4N8IZAyZZT7"},"outputs":[],"source":["tokenizer.save_pretrained(\"directory_on_my_computer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFqFhZpZZZT8","outputId":"0d56959d-dc7f-4c5b-b4b2-b9af0ae80e4e"},"outputs":[{"data":{"text/plain":["['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sequence = \"Using a Transformer network is simple\"\n","tokens = tokenizer.tokenize(sequence)\n","\n","print(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wLaCxuuZZT8","outputId":"bcf4bb89-addf-4877-9b34-59c8a8127b5c"},"outputs":[{"data":{"text/plain":["[7993, 170, 11303, 1200, 2443, 1110, 3014]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6AAmz56ZZT9","outputId":"915678b7-1d20-4ec3-a841-76c12911b324"},"outputs":[{"data":{"text/plain":["'Using a Transformer network is simple'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n","print(decoded_string)"]},{"cell_type":"markdown","metadata":{"id":"Gps0-p_0Zbme"},"source":["# Handling multiple sequences (PyTorch)"]},{"cell_type":"markdown","source":["## Practice"],"metadata":{"id":"TWX-d1gebxRV"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]"],"metadata":{"id":"WVWQiDsxby7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for you my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","input_ids = torch.tensor(ids)\n","\n","# Error! Because models expect multiple sentences by default.\n","model(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"bDvU6hHLd1zR","executionInfo":{"status":"error","timestamp":1700832546845,"user_tz":-540,"elapsed":2493,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"caa2fbaf-cede-44fe-8b50-f53709b31554"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-77-1a22dcbd6ec2>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Error! Because models expect multiple sentences by default.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mwarn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4115\u001b[0m         \u001b[0;31m# Check only the first and last input IDs to reduce overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4116\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4117\u001b[0m             warn_string = (\n\u001b[1;32m   4118\u001b[0m                 \u001b[0;34m\"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"]}]},{"cell_type":"code","source":["tokenized_inputs = tokenizer(sequence, return_tensors='pt')\n","print(tokenized_inputs[\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iy5uEsr0gfED","executionInfo":{"status":"ok","timestamp":1700832628866,"user_tz":-540,"elapsed":2739,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"ea98052b-94e5-4462-f7eb-da7cb0e72a74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 2017, 2026, 2878, 2166, 1012,\n","          102]])\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for the acceptance results of the graduate school.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","input_ids = torch.tensor([ids])\n","print(\"Input IDs:\", input_ids)\n","\n","output = model(input_ids)\n","print(\"Logits:\", output.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87iApWAxhmyw","executionInfo":{"status":"ok","timestamp":1700832978923,"user_tz":-540,"elapsed":4131,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"4073f2ea-e375-4a86-f2b8-24f60ac2a36e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs: tensor([[1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619,\n","         2082, 1012]])\n","Logits: tensor([[ 4.2733, -3.4422]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["batched_ids = [\n","    [150, 150, 150],\n","    [150, 150]\n","]"],"metadata":{"id":"GsDg-XDfjBF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["padding_id = 100\n","\n","batched_ids = [\n","    [150, 150, 150],\n","    [150, 150, padding_id]\n","]"],"metadata":{"id":"bHSDOUaxjrQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence1_ids = [[150, 150, 150]]\n","sequence2_ids = [[150, 150]]\n","batched_ids = [\n","    [150, 150, 150],\n","    [150, 150, tokenizer.pad_token_id],\n","]\n","\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids)).logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_VUuoQBj3Ul","executionInfo":{"status":"ok","timestamp":1700833355639,"user_tz":-540,"elapsed":1903,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"0407a696-e055-4305-a811-492c161543c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.9764, -0.9153]], grad_fn=<AddmmBackward0>)\n","tensor([[ 0.2878, -0.1887]], grad_fn=<AddmmBackward0>)\n","tensor([[ 0.9765, -0.9153],\n","        [ 0.9200, -0.8776]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["batched_ids = [\n","    [150, 150, 150],\n","    [150, 150, tokenizer.pad_token_id],\n","]\n","\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0],\n","]\n","outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n","print(outputs.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PPL51dYkXXf","executionInfo":{"status":"ok","timestamp":1700833630070,"user_tz":-540,"elapsed":639,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"46bbbe4b-9026-4256-8bfc-831ab4512a58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.9765, -0.9153],\n","        [ 0.2878, -0.1887]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["max_sequence_length = 20\n","sequence = sequence[:max_sequence_length]\n","print(sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzaJNCHklv8E","executionInfo":{"status":"ok","timestamp":1700833782968,"user_tz":-540,"elapsed":4,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"6acd8006-46a5-44f3-ff55-1f0d4bad3194"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I've been waiting fo\n"]}]},{"cell_type":"code","source":["tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cq-opdFjuGMT","executionInfo":{"status":"ok","timestamp":1700835917337,"user_tz":-540,"elapsed":2597,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"77736e98-770c-491a-cca8-37b196423684"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"metadata":{},"execution_count":134}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = [\n","    \"I've been walking with my dog.\",\n","    \"I don't have this!\"\n","]\n","\n","tokens0 = tokenizer.tokenize(sequence[0])\n","tokens0.insert(0, \"[CLS]\")\n","tokens0.append(\"[SEP]\")\n","tokens1 = tokenizer.tokenize(sequence[1])\n","tokens1.insert(0, \"[CLS]\")\n","tokens1.append(\"[SEP]\")\n","\n","ids0 = tokenizer.convert_tokens_to_ids(tokens0)\n","ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n","batched_ids = [\n","    ids0,\n","    ids1\n","]\n","batched_ids[1].extend([tokenizer.pad_token_id,tokenizer.pad_token_id])\n","\n","# input_ids0 = torch.tensor([ids0])\n","# input_ids1 = torch.tensor([ids1])\n","input_ids = torch.tensor(batched_ids)\n","\n","# output1 = model(input_ids0)\n","# output0 = model(input_ids1)\n","# print(output0.logits)\n","# print(output1.logits)\n","attention_mask = [\n","    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","    [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n","]\n","\n","output = model(input_ids, attention_mask=torch.tensor(attention_mask))\n","print(output.logits)\n","\n","tokens = tokenizer(sequence, padding=True, truncation=True, return_tensors='pt')\n","print(tokens)\n","print(tokenizer.decode([101, 102]))\n","outputs = model(**tokens)\n","print(outputs.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BpfrHK-mBsy","executionInfo":{"status":"ok","timestamp":1700836306026,"user_tz":-540,"elapsed":2421,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"330bdc56-8d2c-44bf-cdf8-2718ada891ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.6941, -1.3690],\n","        [ 4.1271, -3.2643]], grad_fn=<AddmmBackward0>)\n","{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3788, 2007, 2026, 3899, 1012,  102],\n","        [ 101, 1045, 2123, 1005, 1056, 2031, 2023,  999,  102,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n","[CLS] [SEP]\n","tensor([[ 1.6941, -1.3690],\n","        [ 4.1271, -3.2643]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Example"],"metadata":{"id":"m3nnx0EfAWEQ"}},{"cell_type":"markdown","metadata":{"id":"yqOhjGEAZbmi"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pY4jSG_Zbmi"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPjivXLnZbmj","outputId":"3c023581-8cc3-41b3-d4f8-1941f3526a7d"},"outputs":[{"data":{"text/plain":["IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","input_ids = torch.tensor(ids)\n","# This line will fail.\n","model(input_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lYjQf1JZbml","outputId":"31731245-5ab7-46df-c446-6e5e9678a765"},"outputs":[{"data":{"text/plain":["tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n","          2607,  2026,  2878,  2166,  1012,   102]])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n","print(tokenized_inputs[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yDGPzp7Zbml","outputId":"26e40f16-5c98-40c6-b457-9677d4253f52"},"outputs":[{"data":{"text/plain":["Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n","Logits: [[-2.7276,  2.8789]]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","input_ids = torch.tensor([ids])\n","print(\"Input IDs:\", input_ids)\n","\n","output = model(input_ids)\n","print(\"Logits:\", output.logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVpq2yN2Zbml"},"outputs":[],"source":["batched_ids = [\n","    [200, 200, 200],\n","    [200, 200]\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOCWUBEwZbmm"},"outputs":[],"source":["padding_id = 100\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, padding_id],\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrTRVB6gZbmm","outputId":"7a60b902-19dc-43c5-e77d-62c535c277a3"},"outputs":[{"data":{"text/plain":["tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n","tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n","tensor([[ 1.5694, -1.3895],\n","        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids)).logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiVK_NEVZbmn","outputId":"c24fa402-efce-4f4b-9cf3-c128e94aaf4a"},"outputs":[{"data":{"text/plain":["tensor([[ 1.5694, -1.3895],\n","        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0],\n","]\n","\n","outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n","print(outputs.logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5jnBiz8Zbmn"},"outputs":[],"source":["sequence = sequence[:max_sequence_length]"]},{"cell_type":"markdown","metadata":{"id":"GPccRE9JZdfn"},"source":["# Putting it all together (PyTorch)"]},{"cell_type":"markdown","source":["## Practice"],"metadata":{"id":"APJWwpHFHIO1"}},{"cell_type":"code","source":["!pip install datasets evaluate transformers[sentencepiece]"],"metadata":{"id":"psjXOkDdHJx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for the acceptance results of the graduate school.\"\n","\n","model_inputs = tokenizer(sequence)"],"metadata":{"id":"HsLMxChPJRx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequence = \"I've been waiting for the acceptance results of the graduate school.\"\n","\n","model_inputs = tokenizer(sequence)\n","print(model_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcNOtWt1J6Z_","executionInfo":{"status":"ok","timestamp":1700843252758,"user_tz":-540,"elapsed":276,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"ff25d113-7f75-424a-af14-97be3fc479fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}]},{"cell_type":"code","source":["sequences = [\"I've been waiting for the acceptance results of the graduate school.\", \"I don't hate this procedure!\"]\n","\n","model_inputs = tokenizer(sequences)\n","print(model_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRzG71UYJ8AC","executionInfo":{"status":"ok","timestamp":1700844307420,"user_tz":-540,"elapsed":305,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"821f5238-32da-466b-ba5d-e70281a58299"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012, 102], [101, 1045, 2123, 1005, 1056, 5223, 2023, 7709, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}]},{"cell_type":"code","source":["model_inputs = tokenizer(sequences, padding=\"longest\")\n","print(model_inputs)\n","\n","model_inputs = tokenizer(sequences, padding=\"max_length\")\n","print(model_inputs)\n","\n","model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=4)\n","print(model_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-OKvhZXKDRN","executionInfo":{"status":"ok","timestamp":1700844309301,"user_tz":-540,"elapsed":3,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"0754d1b9-5b5f-4b9d-fb31-31f1f57bcd2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012, 102], [101, 1045, 2123, 1005, 1056, 5223, 2023, 7709, 999, 102, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}\n","{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2123, 1005, 1056, 5223, 2023, 7709, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n","{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012, 102], [101, 1045, 2123, 1005, 1056, 5223, 2023, 7709, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}]},{"cell_type":"code","source":["sequences = [\"I've been waiting for the acceptance results of the graduate school.\", \"I don't hate this procedure!\"]\n","\n","model_inputs = tokenizer(sequences, truncation=True)\n","print(model_inputs)\n","\n","model_inputs = tokenizer(sequences, truncation=True, max_length=4)\n","print(model_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qp78h8FjLJOG","executionInfo":{"status":"ok","timestamp":1700844317489,"user_tz":-540,"elapsed":286,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"2540e6d5-ee1d-45f6-b80a-6c440eb79e18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012, 102], [101, 1045, 2123, 1005, 1056, 5223, 2023, 7709, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","{'input_ids': [[101, 1045, 1005, 102], [101, 1045, 2123, 102]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1]]}\n"]}]},{"cell_type":"code","source":["sequences = [\"I've been waiting for the acceptance results of the graduate school.\", \"I don't hate this procedure!\"]\n","\n","model_inputs = tokenizer(sequences, padding=True, return_tensors='pt')\n","print(model_inputs)\n","\n","model_inputs = tokenizer(sequences, padding=True, return_tensors='tf')\n","print(model_inputs)\n","\n","model_inputs = tokenizer(sequences, padding=True, return_tensors='np')\n","print(model_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shmYT0f8L7Yq","executionInfo":{"status":"ok","timestamp":1700844324204,"user_tz":-540,"elapsed":3,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"e6d2ce7e-f9a0-4ee9-df04-16c7b1a5a046"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996,\n","         4619, 2082, 1012,  102],\n","        [ 101, 1045, 2123, 1005, 1056, 5223, 2023, 7709,  999,  102,    0,    0,\n","            0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n","{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n","array([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997,\n","        1996, 4619, 2082, 1012,  102],\n","       [ 101, 1045, 2123, 1005, 1056, 5223, 2023, 7709,  999,  102,    0,\n","           0,    0,    0,    0,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n","array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n","{'input_ids': array([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997,\n","        1996, 4619, 2082, 1012,  102],\n","       [ 101, 1045, 2123, 1005, 1056, 5223, 2023, 7709,  999,  102,    0,\n","           0,    0,    0,    0,    0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"]}]},{"cell_type":"code","source":["sequence = \"I've been waiting for the acceptance results of the graduate school.\"\n","\n","model_inputs = tokenizer(sequence)\n","print(model_inputs[\"input_ids\"])\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6G_cvTMMVe4","executionInfo":{"status":"ok","timestamp":1700844449927,"user_tz":-540,"elapsed":290,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"c38ad167-d70a-499e-9e0c-55d67363e228"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012, 102]\n","[1045, 1005, 2310, 2042, 3403, 2005, 1996, 9920, 3463, 1997, 1996, 4619, 2082, 1012]\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(model_inputs[\"input_ids\"]))\n","print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Anigq4w3M9UU","executionInfo":{"status":"ok","timestamp":1700844044240,"user_tz":-540,"elapsed":5,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"91447d82-ef02-4641-baf7-e9e6ccaa5ae7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] i've been waiting for the acceptance results of the graduate school. [SEP]\n","i've been waiting for the acceptance results of the graduate school.\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequences = [\"I've been waiting for the acceptance results of the graduate school.\", \"So have I!\"]\n","\n","model_inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors='pt')\n","output = model(**model_inputs)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GtNtqBSkNCff","executionInfo":{"status":"ok","timestamp":1700844462078,"user_tz":-540,"elapsed":1792,"user":{"displayName":"반달곰","userId":"09517991737165413775"}},"outputId":"21881a89-39f1-4beb-fb72-da4d91ffd010"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7752, -3.0598],\n","        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"]}]},{"cell_type":"markdown","source":["## Example"],"metadata":{"id":"1B5TSiN0AXmR"}},{"cell_type":"markdown","metadata":{"id":"fuU_KQNVZdfr"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xtZnKReZdfs"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSsnsuQdZdft"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLz7UvIBZdfu"},"outputs":[],"source":["sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ5YUBxlZdfu"},"outputs":[],"source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","model_inputs = tokenizer(sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_VGd-hIZdfu"},"outputs":[],"source":["# Will pad the sequences up to the maximum sequence length\n","model_inputs = tokenizer(sequences, padding=\"longest\")\n","\n","# Will pad the sequences up to the model max length\n","# (512 for BERT or DistilBERT)\n","model_inputs = tokenizer(sequences, padding=\"max_length\")\n","\n","# Will pad the sequences up to the specified max length\n","model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0NqrVYTZdfv"},"outputs":[],"source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# Will truncate the sequences that are longer than the model max length\n","# (512 for BERT or DistilBERT)\n","model_inputs = tokenizer(sequences, truncation=True)\n","\n","# Will truncate the sequences that are longer than the specified max length\n","model_inputs = tokenizer(sequences, max_length=8, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uX8EWLiZdfv"},"outputs":[],"source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# Returns PyTorch tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n","\n","# Returns TensorFlow tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n","\n","# Returns NumPy arrays\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFtOyYMKZdfw","outputId":"a2df5207-a097-4717-821f-b4a1069b37e9"},"outputs":[{"data":{"text/plain":["[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n","[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)\n","print(model_inputs[\"input_ids\"])\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoARnI9IZdfx","outputId":"cb247693-de8f-40fb-c103-f180e236332b"},"outputs":[{"data":{"text/plain":["\"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\"\n","\"i've been waiting for a huggingface course my whole life.\""]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(tokenizer.decode(model_inputs[\"input_ids\"]))\n","print(tokenizer.decode(ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC-_6gdoZdfx"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","output = model(**tokens)"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb","timestamp":1700390593296}],"collapsed_sections":["VKEx61-5JW3k","F9e7ZdTO_wcd","fT1vAdGZZNsn","ZT0WO2-38TSc","6O9hyBnHAQeE","2H2RFdqlZZT0","k75IvrBAUL5f","gSWJqxMPAUfx","Gps0-p_0Zbme","TWX-d1gebxRV","m3nnx0EfAWEQ","APJWwpHFHIO1","1B5TSiN0AXmR"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}